% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train.R
\name{train}
\alias{train}
\alias{train.HMM}
\alias{train.PHMM}
\title{Iterative refinement of model parameters.}
\usage{
train(x, y, method = "Viterbi", maxiter = 100, logspace = "autodetect",
  quiet = FALSE, deltaLL = 1e-07, modelend = FALSE,
  pseudocounts = "Laplace", gapchar = "-", fixqa = FALSE, fixqe = FALSE,
  inserts = "map", threshold = 0.5, lambda = 0, DI = TRUE, ID = TRUE)

\method{train}{PHMM}(x, y, method = "Viterbi", maxiter = 100,
  logspace = "autodetect", quiet = FALSE, deltaLL = 1e-07,
  pseudocounts = "background", gapchar = "-", fixqa = FALSE,
  fixqe = FALSE, inserts = "map", threshold = 0.5, lambda = 0,
  DI = TRUE, ID = TRUE)

\method{train}{HMM}(x, y, method = "Viterbi", maxiter = 100,
  deltaLL = 1e-07, logspace = "autodetect", quiet = FALSE,
  modelend = FALSE, pseudocounts = "Laplace")
}
\arguments{
\item{x}{an object of class \code{'HMM'} or \code{'PHMM'} specifying the
starting parameter values.}

\item{y}{a list of training sequences (character vectors) whose hidden states are unknown.}

\item{method}{a character string specifying the iterative model training method to use.
Must be set to either \code{method = "Viterbi"} (default) or \code{method = "BaumWelch"}.}

\item{maxiter}{the maximum number of EM iterations before the cycling process is terminated
with an error.}

\item{logspace}{logical argument indicating whether the emission and transition
probabilities of x are logged (base e; TRUE) or raw (FALSE). Alternatively, if
\code{logspace = "autodetect"} (default), the function will automatically detect
if the probabilities are in log space, returning an error if inconsistencies are found.
Note that choosing the latter option increases the computational
overhead; therefore specifying \code{TRUE} or \code{FALSE} can reduce the running time for
large models with many parameters.}

\item{quiet}{logical argument indicating whether the iteration progress should
be printed to the console (\code{quiet = FALSE}; default) or suppressed
(\code{quiet = TRUE}).}

\item{deltaLL}{a numeric value giving the change in log likelihood to specify as the convergence
threshold. Only applicable if \code{method = "BaumWelch"}}

\item{pseudocounts}{used to account for the possible absence of certain transition
and/or emission types in the training dataset.
either \code{'Laplace'} (adds one of each possible transition and emission type to the
training dataset; default), \code{'none'}, or a two-element list containing a matrix of
transition pseudocounts as its first element and a matrix of emission pseudocounts
as its second. If a list is supplied both matrices must have row and column names
according to the residues (column names of emission matrix) and states
(row and column names of the transition matrix and row names of the emission matrix).
The first row and column of the transition matrix must be 'BeginEnd'. Background
pseudocounts are recommended for small training sets,
since Laplacian counts can overinflate insert and delete transition probabilities
leading to convergence at suboptimal local maxima.}
}
\description{
Update model parameters using a list of training sequences. Methods available include
Viterbi training (also known as the segmental K-means algorithm (Juang & Rabiner 1990)),
and the Baum Welch algorithm, a special case of the expectation-maximization (EM) algorithm
that iteratively finds the local (but not necessarily global) optimal parameters of a HMM or PHMM.
}
\references{
Juang B-H & Rabiner L R (1990) The segmental K-means
algorithm for estimating parameters of hidden Markov models.
IEEE transactions on Acoustics,
Speech...

Durbin...
}
\seealso{
\code{\link{deriveHMM}} and \code{\link{derivePHMM}} for
maximum-likelihood parameter estimation when training sequence states are
known.
}

